---
title: "Course logistics"
subtitle: "General linear model"  
author: 
  - "Dr Nemanja Vaci"
date: '`r Sys.Date()`'
output:
  xaringan::moon_reader:
    css: xaringan-themer.css
    nature:
      slideNumberFormat: "%current%"
      highlightStyle: github
      highlightLines: true
      ratio: 16:9
      countIncrementalSlides: true

---
Press record

```{r setup, include=FALSE}
options(htmltools.dir.version = FALSE)
knitr::opts_chunk$set(
  fig.width=9, fig.height=3.5, fig.retina=3,
  out.width = "100%",
  cache = FALSE,
  echo = TRUE,
  message = FALSE, 
  warning = FALSE,
  hiline = TRUE
)
```

```{r xaringan-themer, include=FALSE, warning=FALSE}
library(xaringanthemer)
style_mono_accent(
  base_color = "#440099",
  header_font_google = google_font("Helvetica"),
  text_font_google   = google_font("Helvetica", "300", "300i"),
  code_font_google   = google_font("Fira Code"),
  code_font_size = '1.1rem',
  text_color = 'black',
  text_font_size = '24px',
  colors = c(red = "#f34213",
             green = "#136f63")
)
```

---
## Logistics - theory

- What are we planning to talk about : https://nemanjavaci.netlify.app/advanced-stats-course/course-handbook_2024/<br/> <br/>
  a) Generalized linear models <br/>
  b) Structural equation modelling: path models and confirmatory factor analysis <br/>
  c) Mixed-effects models: cross-sectional and longitudinal data <br/> <br/>
- Theory and practice (70 minutes + 30 minutes):<br/><br/>
a) Theoretical aspect - why and when would we want to use a certain statistical model <br/>
b) Mathematical aspect - the mathematical basis of the model and how can we transform the data and parameters <br/>
c) Practical aspect - using R to analyse the data and build the statistical models on real-world data <br/>
<br/>
---

## Logistics - practice

We will start each lecture with the analysis of the data using models from previous lecture: this is going to be led by you! <br/>
<br/>
- R statistical environment
<br/><br/><br/>
- Materials:<br/>
a) Presentations (press __p__ for additional content)<br/>
b) Commented R code
([link](http://localhost:6226/advanced-stats-course/rcode_23/) for this lecture)<br/>
c) Course glossary: [link](https://nemanjavaci.netlify.app/advanced-stats-course/psy6210-glossary/)
???
General overview of the course<br/>
<br/>
Focus on theory and applications, but also on how to use these models in the R environment<br/>
<br/>
Each lecture is planned to be supported by HTML presentation with main slide content and lecturer's notes + commented code that produced outputs in the presentation. Often, I will include additional content that you can explore, but it is not going to be used for any subsequent knowledge assessments. 
<br/>
I will provide you the data and research question, and your job will be to analyse the data and prepare a short discussion for the next meeting. 
---
## Knowledge assessment

####The application of statistical methods to the existing data
A series of research questions for which you will have to propose a statistical model that tests hypothetical assumptions, motivate your models, build them in the R statistical environment, and interpret results (parameters, fit and critique of the model)

Homework: 10% of the final mark <br/>
Final essay type exam: 90% of the final mark <br/><br/>


The exam will focus on the: <br/>
a) Theoretical aspect <br/>
b) Mathematical aspect <br/>
c) Practical aspect 
<br/>
---
## Main literature

.pull-left[
Gelman, A., & Hill, J. (2006). Data analysis using regression and multilevel/hierarchical models. Cambridge university press.]
.pull-right[
<img src="title.jpg", width = "40%">
]

Hayes, A. (2018). Introduction to Mediation, Moderation, and Conditional Process Analysis: A Regression-Based Approach <br/>

Principles and Practice of Structural Equation Modeling by Rex B. Kline <br/>
???
Additional literature: <br/>
1. McElreath, R. (2018). Statistical rethinking: A Bayesian course with examples in R and Stan. Chapman and Hall/CRC.
<br/>
2. Navarro, D. R for Psychological Sciences (https://psyr.djnavarro.net/)
<br/>
3. DeBruine, L. M., & Barr, D. J. (2021). Understanding Mixed-Effects Models Through Data Simulation. Advances in Methods and Practices in Psychological Science, 4(1), 2515245920965119.
<br/>
4. Wang, Y. A., & Rhemtulla, M. (2021). Power analysis for parameter estimation in structural equation modeling: A discussion and tutorial. Advances in Methods and Practices in Psychological Science, 4(1), 2515245920918253.
<br/>
5. Lafit, G., Adolf, J., Dejonckheere, E., Myin-Germeys, I., Viechtbauer, W., & Ceulemans, E. (2020). Selection of the number of participants in intensive longitudinal studies: A user-friendly Shiny app and tutorial to perform power analysis in multilevel regression models that account for temporal dependencies.
<br/>
6. Summary songs by Rafael Moral (https://www.youtube.com/user/rafamoral2007)

---
## Opportunities for feedback:

Discussions during the classes <br/>
Feedback on the homework <br/>
Feedback form <br/>

Course communication: n.vaci@sheffield.ac.uk <br/>
Office hours: Friday from 1 to 2 pm (https://tinyurl.com/y2ptqmwx)<br/>
Post your questions on the spreadsheet queries

---
## Today: Linear model

Intended learning outcomes: <br/>

1. Explain and motivate the linear regression model <br/> <br/> 

2. Build a model; analyse and interpret the coefficients (model with one predictor, categorical predictors, multiple predictors, and interactions) <br/><br/> 

3. Interpret other information that linear regression reports: determination coefficient, F-test, residuals <br/><br/> 

4. Evaluate fit and assumptions of the linear regression model <br/>

---
class: center, inverse
background-image: url("main.gif")
---

## Gaussian distribution

$$y_ \sim \mathcal{N}(\mu,\sigma)$$
```{r, fig.align='center', echo=FALSE, out.width='90%'}
knitr::include_graphics('normalD.png')
```
???
Gaussian distribution is described with two parameters: mean (measure of central tendency) and dispersion around the mean (standard deviation). 

Central limit theorem: [link](https://en.wikipedia.org/wiki/Central_limit_theorem)<br/><br/>

What can we do with one distribution: [Moments](https://en.wikipedia.org/wiki/Moment_(mathematics), [Normality](https://en.wikipedia.org/wiki/Normality_test#:~:text=In%20statistics%2C%20normality%20tests%20are,set%20to%20be%20normally%20distributed.)<br/>
What can we do with two? With three or more? <br/>

---

## Linear regression

Method that summarises how the **.red[average values]** of numerical outcome variable vary over values defined by a .white.bg-green[linear function of predictors] ([visualisation](https://seeing-theory.brown.edu/regression-analysis/index.html#section1)) <br/> <br/>

--

$$ y = \alpha + \beta * x + \epsilon $$
--
```{r,fig.retina=8,fig.height=3.4,fig.align='center', echo=FALSE}
par(bty='n',mar = c(5, 4, .1, .1), cex=1.1, pch=16)
data("cars")
plot(cars$speed, cars$dist, xlab='Predictor', ylab='Outcome')
abline(lm(dist~speed, data=cars), lwd=2)
```
???
The mathematical equation that estimates a dependent variable Y from a set of predictor variables or regressors X.<br/>

Each regressor gets a weight/coefficient that is used to estimate Y. This coefficient is estimated according to some criteria, eg. minimises the sum of squared errors or maximises the logarithm of the likelihood function. Additional reading: <br/> [Estimation of the parameters](https://data.princeton.edu/wws509/notes/c2s2) 
<br/><br/>

$\epsilon$ - measure of accuracy of the model<br/>
$SS_{residual}=\sum_{i=1}^{n}(Y_{i}-\hat{Y}_{i})^2=\sum_{i=1}^{n}e_{i}^2$<br/>
<br/>
Other options: https://en.wikipedia.org/wiki/Deming_regression

---
## Looking back at Gaussian distribution

$$y_ \sim \mathcal{N}(\mu,\sigma)$$
--
```{r,fig.retina=8,fig.height=3.4,fig.align='center', echo=FALSE}
par(bty='n',mar = c(5, 4, .1, .1), cex=1.1, pch=16)
data("cars")
plot(cars$speed, cars$dist, xlab='Predictor', ylab='Outcome')
abline(lm(dist~speed, data=cars), lwd=2)
```

$$ y =\mathcal{N}(\alpha + \beta * x,\sigma) $$

---
## Let's get some data

--
Model the **.red[height]** (cm) of babies as a function of age (months), weight (grams) and sex:

```{r,echo=FALSE}
set.seed(456)
Babies=data.frame(Age=round(runif(100,1,30)), Weight=rnorm(100,4000,500))
Babies$Height=rnorm(100,40+0.2*Babies$Age+0.004*Babies$Weight, 5)
Babies$Sex=factor(rbinom(100,1,0.5))
levels(Babies$Sex)=c('Girls','Boys')
```

```{r,echo=FALSE, fig.retina=8,fig.height=3.8,fig.align='center'}
par(mfrow=c(1,3), bty='n',mar = c(5, 4, .1, .1), cex=1.1, pch=16)
plot(Babies$Age, Babies$Height, xlab='Age (months)', ylab='Height (cm)')
plot(Babies$Weight, Babies$Height, xlab='Weight (grams)', ylab='Height (cm)')
boxplot(Babies$Height~Babies$Sex,xlab='Sex', ylab='Height (cm)')
```
???
Code for the simulation of the data:
```{r,echo=T}
set.seed(456)
Babies=data.frame(Age=round(runif(100,1,30)), Weight=rnorm(100,4000,500))
Babies$Height=rnorm(100,40+0.2*Babies$Age+0.004*Babies$Weight, 5)
Babies$Sex=factor(rbinom(100,1,0.5))
levels(Babies$Sex)=c('Girls','Boys')
```
---
## Linear regression: one predictor

$$ y = \alpha + \beta * Age + \epsilon $$ 

How would you do that in R? <br/>

--

```{r}
lm1 <- lm( Height ~ Age, data = Babies )
lm1 $ coefficients
```
--
Intercept is the **.red[average predicted height]** for children at **.green[birth]** (0 - months)<br/> <br/>

Counterfactual interpretation (causal): **.red[Increase of 1 unit]** in predictor value - Age (being one month older) changes the outcome by the **.green[size]** of the $\beta$ (model estimated value) <br/> <br/>

Predictive interpretation (descriptive): If we compare babies that **.red[differ]** in their age by 1 month, we expect to see that older babies are taller by $\beta$ on **.green[average]** <br/> <br/>

???
Model summarises the difference in average Height between the children as a function of their Age. <br/> <br/>

Type of interpretation usually depends on the strength of the theoretical framework and methodological design. If you have well developed theoretical assumptions and experimental design, this perhaps allows you to use counterfactual interpretation.  

---
## Interpretation: multiple predictors

$$y = \alpha + \beta_1 * Age + \beta_2 * Weight + \epsilon$$ 
Interpretation becomes contingent on other variables in the model <br/> <br/>

--

```{r}
lm2 <- lm ( Height ~ Age + Weight, data = Babies )
lm2 $ coefficients
```

Age: Comparing babies that have **.red[same Weight]**, but differ in their **.red[Age by one month]**, the model predicts difference by a value of $\beta_1$ in their Height on average<br/> <br/>

Weight: Comparing babies that have **.red[same Age]**, but differ in their **.red[Weight by one gram]**, the model predicts difference by a value of $\beta_2$ in their Height on average

Regression coefficient is a [partial correlation](https://en.wikipedia.org/wiki/Partial_correlation) estimate 

???
```{r}
AgeRes=residuals(lm(Age~Weight, data=Babies))
HeightRes=residuals(lm(Height~Weight, data=Babies))
lm(HeightRes~AgeRes)$coefficients
```
---
## Interpretation: multiple predictors

$$y = \alpha + \beta_1 * Age + \beta_2 * Sex + \epsilon$$ 

Interpretation becomes contingent on other variables in the model <br/> <br/>
--
```{r}
lm2 <- lm( Height ~ Age + Sex, data = Babies )
lm2 $ coefficients
```

Age: Comparing babies that have **.red[identical Sex]**, but differ in their **.red[Age by one month]**, the model predicts difference by a value of $\beta_1$ in their Height on average<br/> <br/>

Sex: Comparing babies that have **.red[same Age]**, but have **.red[different Sex]**, the model predicts difference by a value of $\beta_2$ in their Height on average <br/> <br/>

???
The model summarises difference in average Height between the children as a function of their Age and Sex. <br/> <br/>

Intercept is the average (prediction) score for girls (0 on Sex variable) and that have 0 months (birth).
---

## Categorical predictors

$$y = \alpha + \beta_1 * Age + \beta_2 * Sex_{Boys} + \epsilon$$ 

What is the model doing:
.center[
<img src="CatPred.png", width = "30%">
]


Each level is assigned a value: Girls - 0, Boys - 1 <br/> <br/>
The slope coefficient $\beta_2$ models the difference between the two levels

???
Additional reading on type of dummy coding in R: [link](https://stats.idre.ucla.edu/r/modules/coding-for-categorical-variables-in-regression-models/)
---
## Interpretation: interactions 1

$$y = \alpha + \beta_1 * Age + \beta_2 * Sex_{Boys} + \beta_3 * Age*Sex_{Boys} + \epsilon$$ 

We allow the slope of age to linearly change across the subgroups of Sex variable<br/>

--

```{r}
lm3 <- lm ( Height ~ Age * Sex, data = Babies )
lm3 $ coefficients
```

Age: In the case of **.red[girls]**, comparing babies **.red[older by a month]**, the model predicts average difference by $\beta_1$ coefficient<br/>

Sex: Expected **.red[average difference]** between girls at birth and boys at birth is $\beta_2$ coefficient<br/> <br/>

Age X Sex: Difference in the **.red[Age slope]** between Girls and Boys
???
Intercept is the predicted Girls Hight at birth <br/> <br/>
Predicted Height for Boys, when Age is 0: Intercept + $Sex_{boys}$ <br/> <br/>
Slope for the Boys: Age + Age: $Sex_{Boys}$
---
## Visualisation: interactions 1 

```{r, echo=F, fig.width=12, fig.height=6, fig.align='center', warning=FALSE, message=FALSE}
require(sjPlot)
plot_model(lm3, terms=c('Age','Sex'), type='pred')
```

---
## Interpretation: interactions 2

What about by-linear continous interactions?

$$y = \alpha + \beta_1 * Age + \beta_2 * Weight + \beta_3 * Age*Weight + \epsilon$$ 

--

```{r}
lm4<-lm(Height~Age*Weight, data=Babies)
lm4$coefficients
```

Age: Keeping **.red[weight at 0]** and comparing babies that **.green[differ by 1 month]** in their age, the model predicts average difference of $\beta_1$ 

Weight: Keeping age at **.red[0 (birth)]** and comparing babies that **.green[differ by 1 gram]** in their weight, the model predicts average difference of $\beta_2$

Age X Weight: The **.red[average difference]** between babies that **.green[differ by 1 month]** in their age, changes by $\beta_3$ with every **.red[1 gram change]** in babies weight

???
By including an interaction we allow a model to be fit differently to subsets of data.<br/> <br/>
When to test the interactions: does the theory/clinical practice/previous studies postulate possibilities of interaction existing between your main effects? If yes, then proceed with caution. <br/> <br/>
+ a good sign is when the main effects are relatively strong and large and they explain a large amount of variation in the data

---

## Visualisation of the interaction

```{r, echo=F, fig.width=14, fig.height=5, fig.align='center', warning=FALSE, message=FALSE}
require(ggplot2)
simulatedD<-data.frame(Age=rep(seq(0, 30, by=1), 13), Weight=rep(seq(2872,4500, by=100), each=403))
simulatedD$Pred=predict(lm4, newdata = simulatedD)
p<-ggplot(simulatedD, aes(Age,Pred, color=Weight,frame=Weight))+geom_line()+ylab('Predicted height')
plotly::ggplotly(p)
```

---

## Additional information: model

```{css, echo=FALSE}
pre {
  max-height: 300px;
  overflow-y: auto;
}

pre[class] {
  max-height: 100px;
}
```

```{css, echo=FALSE}
.scroll-100 {
  max-height: 100px;
  overflow-y: auto;
  background-color: inherit;
}
```

```{r}
lm1<-lm(Height~Age, data=Babies)
summary(lm1)
```

???
Residuals - estimate of the error of the model <br/> <br/>
Coefficients with standard errors, as well as t-test and significance values. Test statistic (t-test) is used to test the significance of the predictor against 0. The reason why is it approximated with t distribution: [Link](https://stats.stackexchange.com/a/344008)<br/> <br/>
Residual standard error - estimate of the fit of our model: $\sqrt(\frac{SS_{residual}}{df})$
---

## Determination coefficient

$R^2 = 1 - \frac{SS_{residual}}{SS_{total}}$
--
```{r, echo=FALSE, warning=FALSE, message=FALSE}
require(ggplot2)
Babies$lm1=predict(lm1, newdata = Babies)
Babies$diff=Babies$lm1-Babies$Height
```

```{r,fig.width=12, fig.height=5, fig.align='center', warning=FALSE, echo=FALSE}
ggplot()+geom_linerange(data=Babies,aes(x=Age, ymin=Height,ymax=lm1,colour=diff), size=1.2)+geom_line(data=Babies,aes(x=Age, y=lm1), size=1.2)+geom_point(data=Babies, aes(Age, y=Height), size=2)+ylab('Height')+xlab('Age')+ggtitle('SS_residual')+theme(axis.title=element_text(size=14), axis.text =element_text(size=12))
```
???
Residual sum of squares: calculating vertical distances between observed data points and fitted regression line, raising them to the power of 2 (squaring) and summing them (addition). The residual sum of squares informs us on the unexplained variance in our data - variance in the error term.  
---
## Determination coefficient

$R^2 = 1 - \frac{SS_{residual}}{SS_{total}}$

```{r}
lm0<-lm(Height~1, data=Babies)
summary(lm0)
```

```{r, echo=FALSE}
Babies$lm0=predict(lm0, newdata = Babies)
Babies$diff2=Babies$lm0-Babies$Height
```
???
We can fit only intercept model - only mean level of the dependent variable
---

## Determination coefficient

$R^2 = 1 - \frac{SS_{residual}}{SS_{total}}$

```{r,fig.width=10, fig.width=12, fig.height=5, fig.align='center', warning=FALSE, echo=FALSE}

ggplot()+geom_linerange(data=Babies,aes(x=Age, ymin=Height,ymax=lm0,colour=diff2), size=1.2)+geom_line(data=Babies,aes(x=Age, y=lm0), size=1.2)+geom_point(data=Babies, aes(Age, y=Height), size=2)+ylab('Height')+xlab('Age')+ggtitle('SS_total')+theme(axis.title=element_text(size=14), axis.text =element_text(size=12))
```

???
Total sum of squares: calculating vertical distances between observed data points and null model regression line (only intercept - mean of dependent variable), raising them to the power of 2 (squaring) and summing them (addition). The total sum of squares informs us on the total variance in our outcome/dependent variable. 
---
## Improvement in our prediction?

```{r, echo=FALSE}
Babies$diff3=Babies$lm1-Babies$lm0
```

$F = \frac{SS_m/df_m}{SS_r/df_r}$


```{r,fig.width=12, fig.height=5, fig.align='center', warning=FALSE, echo=FALSE}
ggplot()+geom_linerange(data=Babies,aes(x=Age, ymin=lm1,ymax=lm0,colour=diff3), size=1.2)+geom_line(data=Babies,aes(x=Age, y=lm0), size=1.2)+geom_line(data=Babies, aes(Age, y=lm1), size=1.3, linetype='dotdash')+geom_point(data=Babies, aes(x=Age, y=Height), size=0.9)+ylab('Height')+xlab('Age')+ggtitle('Improvement')+theme(axis.title=element_text(size=14), axis.text =element_text(size=12))
```
???
We can also calculate an F-test by hand. First we would need to calculate the amount of variance in the data that was explained by our model. We would get that by calculating distances between our fitted data points (fitted line) and null model line (mean level of dependent variable). We would square those distances and sum them up to get model/explained sum of squares.<br/><br/><br/>
To get an F-test, we would divide model sum of squares by the degrees of freedom spent on estimating the model (dfmodel - number of predictors). We would divide that by another unexplained amount of variance (residual sum of squares divided by residual degrees of freedom -> n - k +1 ).
---
## Why is this important? 

The general linear model is "basis" for all other models covered by this course <br/>

More complex statistical models are often just generalisations of the general linear model <br/>

Correlations, t-test, ANOVA ,ANCOVA can be expressed as the general linear model <br/>

```{r}
cor(Babies$Height,Babies$Weight)
```

```{r}
Babies$HeightStand=scale(Babies$Height, scale=TRUE, center=TRUE)
Babies$WeightStand=scale(Babies$Weight, scale=TRUE, center=TRUE)
lmCorr<-lm(HeightStand~-1+WeightStand, data=Babies)
lmCorr$coefficients
```
???
General linear model is a special case of the broad family of models (Generalized Linear Models - more next week)
---

## Asumptions

a) Error distribution: $\mathcal{N}^{iid}(0,\sigma^2)$ <br/> <br/>
b) Linearity and additivity <br/> <br/>
c) Validity <br/> <br/>
d) Lack of perfect multicolinearity <br/> <br/>

???
0 being mean of residuals is a by-product of OLS when we include intercept. <br/> Assumptioms of IID ~ independently and identically distributed residuals, are often problematic in psychology, when we have clustered answers. <br/><br/>

Linearity - outcome can be modelled as a linear function of separate predictors<br/>
Additivity - postulated linear function should not vary across values of other variables, if it does - we need to add interactions<br/>
Validity - data that you are measuring should reflect the phenomenon you are interested in<br/>

---


## What other distributions do you know?

$$y_ \sim \mathcal{N}(\mu,\sigma)$$
```{r, fig.align='center', echo=FALSE, out.width='70%'}
knitr::include_graphics('normalD.png')
```

--
Distribution overview: [link](http://www.math.wm.edu/~leemis/chart/UDR/UDR.html)

---
class: inverse, middle, center
# Practical aspect
---

#Practical work
Example dataset: Income inequality and rates of hate crimes -
[Article](https://fivethirtyeight.com/features/higher-rates-of-hate-crimes-are-tied-to-income-inequality/) and 
[Data](https://github.com/fivethirtyeight/data/tree/master/hate-crimes)

Reading local files to R:
```{r}
inequality<-read.table('inequality.txt',sep=',', header=T)
knitr::kable(head(inequality[,c(1,2,8,12)]), format = 'html')
```

???
In order to read/load the data in R, you need to specify where R is supposed to look for the files. One way you can achieve this is by setting up the working directory. You can click on the tab __Session__ then __Set working directory__ then __Choose directory__ and then navigate to folder where your data is. Then you can just follow the code outlined in the presentations and/or in the R code for this lecture. 

## Take away message (important bits)

Linear model - continuous and categorical predictors <br/> <br/>
Interpreting estimated coefficients <br/> <br/>
Interpreting results (significance, determination coefficient, F-test, residuals) <br/> <br/>
Understanding assumptions of linear model <br/> <br/>
Using linear models in R: overall and stepwise approach <br/> <br/>
Model diagnostics based on pre and post-modelling procedures  covered  by the lecture and beyond

---

## Literature

First and second chapter of "Data Analysis Using Regression and Multilevel/Hierarchical Models" by Andrew Gelman and Jennifer Hill <br/> <br/>

First three chapter of "Regression Analysis and Linear Models: Concepts, Applications, and Implementation" by Richard B. Darlington and Andrew F. Hayes <br/> <br/>

van Rij, J., Vaci, N., Wurm, L. H., & Feldman, L. B. (2020). Alternative quantitative methods in psycholinguistics: Implications for theory and design. Word Knowledge and Word Usage, 83-126.

---

## Exercise for the next week

1. Fill the reflection and feedback form by Tuesday: https://forms.gle/DoeBNKZqCeRvSmZ46 <br/><br/>

2. Prepare for discussion for next week! 

---

# Thank you for your attention

[Song for the weekend](https://youtu.be/pZTLFu79UbY)